{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "151a7a29",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8ab29d7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import bs4\n",
    "import json\n",
    "import time\n",
    "from PIL import Image\n",
    "import io\n",
    "import pandas as pd\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "386506a7",
   "metadata": {},
   "source": [
    "# Extract web content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ddb55524",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Request was successful.\n"
     ]
    }
   ],
   "source": [
    "url = 'https://www.tilbudsugen.dk/partner/netto-114?page=100'\n",
    "\n",
    "response = requests.get(url)\n",
    "time.sleep(2)\n",
    "if response.status_code == 200:\n",
    "    print(\"Request was successful.\")\n",
    "else:\n",
    "    assert False, f\"Request failed with status code {response.status_code}\"\n",
    "\n",
    "soup = bs4.BeautifulSoup(response.text, 'html.parser')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "579abfd8",
   "metadata": {},
   "source": [
    "# Get product ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e775aa69",
   "metadata": {},
   "outputs": [],
   "source": [
    "links = soup.find_all('a', href=True)\n",
    "product_links = [link['href'] for link in links if link['href'].startswith('https://www.tilbudsugen.dk/single/')]\n",
    "all_ids = [int(link.split('/')[-1]) for link in product_links]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb79d025",
   "metadata": {},
   "source": [
    "# Get product info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3b3d4abb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing #0 of 393\n",
      "Processing #10 of 393\n",
      "Processing #20 of 393\n",
      "Processing #30 of 393\n",
      "Processing #40 of 393\n",
      "Processing #50 of 393\n",
      "Processing #60 of 393\n",
      "Processing #70 of 393\n",
      "Processing #80 of 393\n",
      "Processing #90 of 393\n",
      "Processing #100 of 393\n",
      "Processing #110 of 393\n",
      "Processing #120 of 393\n",
      "Processing #130 of 393\n",
      "Processing #140 of 393\n",
      "Processing #150 of 393\n",
      "Processing #160 of 393\n",
      "Processing #170 of 393\n",
      "Processing #180 of 393\n",
      "Processing #190 of 393\n",
      "Processing #200 of 393\n",
      "Processing #210 of 393\n",
      "Processing #220 of 393\n",
      "Processing #230 of 393\n",
      "Processing #240 of 393\n",
      "Processing #250 of 393\n",
      "Processing #260 of 393\n",
      "Processing #270 of 393\n",
      "Processing #280 of 393\n",
      "Processing #290 of 393\n",
      "Processing #300 of 393\n",
      "Processing #310 of 393\n",
      "Processing #320 of 393\n",
      "Processing #330 of 393\n",
      "Processing #340 of 393\n",
      "Processing #350 of 393\n",
      "Processing #360 of 393\n",
      "Processing #370 of 393\n",
      "Processing #380 of 393\n",
      "Processing #390 of 393\n"
     ]
    }
   ],
   "source": [
    "products = {}\n",
    "all_ids = all_ids\n",
    "N = len(all_ids)\n",
    "for i, data_id in enumerate(all_ids):\n",
    "    if i % 10 == 0:\n",
    "        print(f\"Processing #{i} of {N}\")\n",
    "    data_url = f\"https://www.tilbudsugen.dk/_next/data/0LbXUdvz48Lb0tgkd4pVT/dk/single/{data_id}.json?id={data_id}\"\n",
    "    response = requests.get(data_url)\n",
    "    time.sleep(1)\n",
    "    if response.status_code != 200:\n",
    "        assert False, f\"Request failed with status code {response.status_code}\"\n",
    "    page_props = response.json()['pageProps']\n",
    "    # print(json.dumps(page_props, indent=2))\n",
    "    \n",
    "    price        = page_props['offer']['price']\n",
    "    brand        = page_props['offer']['brand']['name']\n",
    "    category     = page_props['offer']['productVariant']['category']['name']\n",
    "    product_name = page_props['offer']['productName']['productName']\n",
    "    units        = page_props['offer']['units']\n",
    "    quantity     = int(eval(page_props['offer']['quantity']))\n",
    "    unit_type    = page_props['offer']['unitType']\n",
    "    store_name   = page_props['offer']['chain']['name']\n",
    "    image_url    = page_props['offer']['imageUrl']\n",
    "    start_date   = page_props['offer']['startDate']\n",
    "    end_date     = page_props['offer']['endDate']\n",
    "\n",
    "    products[data_id] = [price, brand, category, product_name, units, quantity, unit_type, store_name, image_url, start_date, end_date]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49fe4fc6",
   "metadata": {},
   "source": [
    "# Gather data in table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e8d5e4d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_products = pd.DataFrame.from_dict(products, orient='index', columns=['price', 'brand', 'category', 'product_name', 'units', 'quantity', 'unit_type', 'store_name', 'image_url', 'start_date', 'end_date'])\n",
    "df_products.reset_index(inplace=True)\n",
    "df_products.rename(columns={'index': 'data_id'}, inplace=True)\n",
    "date = df_products['start_date'].mode()[0]\n",
    "df_products.to_csv(f'../data/csv/products_{date}.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28adf2e8",
   "metadata": {},
   "source": [
    "# Download resized images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "76f330d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_and_resize_image(image_url, data_id, date):\n",
    "    extension = image_url.split(\"?\")[0].split(\".\")[-1].lower()\n",
    "    os.makedirs(f\"../data/imgs/{date}\", exist_ok=True)\n",
    "    filename = f\"../data/imgs/{date}/{data_id}.{extension}\"\n",
    "\n",
    "    resp = requests.get(image_url)\n",
    "    if resp.status_code != 200:\n",
    "        print(\"Failed to download image:\", resp.status_code)\n",
    "    else:\n",
    "        img = Image.open(io.BytesIO(resp.content))\n",
    "        w, h = img.size\n",
    "        max_side = max(w, h)\n",
    "        if max_side > 300:\n",
    "            scale = 300 / max_side\n",
    "            new_size = (int(round(w * scale)), int(round(h * scale)))\n",
    "            try:\n",
    "                resample = Image.Resampling.LANCZOS\n",
    "            except AttributeError:\n",
    "                resample = Image.LANCZOS\n",
    "            img = img.resize(new_size, resample)\n",
    "        img.save(filename)\n",
    "\n",
    "# for all product_ids in df_products, download and resize images\n",
    "for idx, row in df_products.iterrows():\n",
    "    download_and_resize_image(row['image_url'], row['data_id'], date)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "grocery-swiper (3.13.5)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
